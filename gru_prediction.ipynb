{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "gru_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elansarihouria/TEXT_CLASSLFICATION/blob/master/gru_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HSOzzcgXWE2",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "Sentiment analysis probably is one the most common applications in Natural Language processing. I don’t have to emphasize how important customer service tool sentiment analysis has become. So here we are, we will train a classifier movie reviews in IMDB data set, using Recurrent Neural Networks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will use Recurrent Neural Networks, and in particular GRUs, to perform sentiment analysis in Keras. Conveniently, Keras has a built-in IMDb movie reviews data set that we can use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPWSrzoqXWFA",
        "colab_type": "text"
      },
      "source": [
        "# Data Overview######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tnx7gYlXWFI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "For this analysis we’ll be using a dataset of 50,000 movie reviews taken from IMDb. The data was compiled by Andrew Maas and can be found here: IMDb Reviews.\n",
        "\n",
        "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
        "\n",
        "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk77uLv-XWFQ",
        "colab_type": "text"
      },
      "source": [
        "# Framwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aiw5XQOTXWFU",
        "colab_type": "text"
      },
      "source": [
        "Keras is a very popular python deep learning library, similar to TFlearn that allows to create neural networks without writing too much boiler plate code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjfESTyRXWFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvaGdktIXWF8",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OER1b9oaXWGA",
        "colab_type": "text"
      },
      "source": [
        "Set the vocabulary size and load in training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "outfYLhQXWGD",
        "colab_type": "code",
        "outputId": "4345c55f-6e2a-4195-f407-f45e27a13d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary_size = 25000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Zbsn6VXWGV",
        "colab_type": "text"
      },
      "source": [
        "## The code above does a couple of things at once:\n",
        "\n",
        "    It downloads the data\n",
        "    It downloads the first 25000 top words for each review\n",
        "    It splits the data into a test and a training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JmGHcfuXWGY",
        "colab_type": "text"
      },
      "source": [
        "Loaded dataset with 25000 training samples, 25000 test samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnYFA-yWXWGb",
        "colab_type": "code",
        "outputId": "51d5b26e-7213-4ffa-8b02-5ca7e9e1864d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc171g0CXWGt",
        "colab_type": "text"
      },
      "source": [
        "If you look at the data you will realize it has been already pre-processed. All words have been mapped to integers and the integers represent the words sorted by their frequency. This is very common in text analysis to represent a dataset like this. So 4 represents the 4th most used word, 5 the 5th most used word and so on... The integer 1 is reserved reserved for the start marker, the integer 2 for an unknown word and 0 for padding. \n",
        "\n",
        "Inspect a sample review and its label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Zrw7rlXWGv",
        "colab_type": "code",
        "outputId": "cd25b63f-da75-4e43-bc99-65b78577ea4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print('---review---')\n",
        "print(X_train[6])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 16393, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQd73apLXWHA",
        "colab_type": "text"
      },
      "source": [
        "Note that the review is stored as a sequence of integers. These are word IDs that have been pre-assigned to individual words,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JweP8qLFXWHD",
        "colab_type": "text"
      },
      "source": [
        "We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaIan9S7XWHG",
        "colab_type": "code",
        "outputId": "55036db0-a769-4386-c64f-403b9fb2a4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---')\n",
        "print(y_train[6])\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review with words---\n",
            "['the', 'boiled', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'murdering', 'naschy', 'br', 'villain', 'council', 'suggestion', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'echoed', 'concentrates', 'concept', 'issue', 'skeptical', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'starship', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'originals', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'dose', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAp8wZUEXWHU",
        "colab_type": "text"
      },
      "source": [
        "Maximum review length and minimum review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j35GOstEXWHW",
        "colab_type": "code",
        "outputId": "a886b2e2-9746-4f99-9594-b025ba6564a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Maximum review length: {}'.format(\n",
        "len(max((X_train + X_test), key=len))))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 2697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "542eKTFxXWHi",
        "colab_type": "code",
        "outputId": "5a040293-e252-4cec-f07e-c5ab2a94fc1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Minimum review length: {}'.format(\n",
        "len(min((X_train + X_test), key=len))))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum review length: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56l3qRZFXWHx",
        "colab_type": "text"
      },
      "source": [
        "Pad sequences\n",
        "\n",
        "In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this using the pad_sequences() function in Keras. For now, set max_words to 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MaZb4IbXWH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "max_words =200\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiqgJdZWXWIF",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rJ1p5jxXWII",
        "colab_type": "text"
      },
      "source": [
        "Design an RNN model for sentiment analysis\n",
        "\n",
        "We start building our model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
        "\n",
        "Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_words, and our output is a binary sentiment label (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3StGHohgXWIK",
        "colab_type": "code",
        "outputId": "0af38206-e3c0-4f95-e82d-1cf11d8f16bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, GRU, Dense, Dropout\n",
        "embedding_size=32\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(GRU(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 200, 32)           800000    \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 100)               39900     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 840,001\n",
            "Trainable params: 840,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBnWvgEVXWIY",
        "colab_type": "text"
      },
      "source": [
        "The two most important things in our code are the following:\n",
        "\n",
        "    The Embedding layer and\n",
        "    The GRU Layer.\n",
        "\n",
        "Lets cover what both are doing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjGChjU-XWIb",
        "colab_type": "text"
      },
      "source": [
        "## Word embeddings\n",
        "\n",
        "The embedding layer will learn a word embedding for all the words in the dataset. It has three arguments the input_dimension in our case the 200 words. The output dimension aka the vector space in which words will be embedded. In our case we have chosen 32 dimensions so a vector of the length of 32 to hold our word coordinates.\n",
        "\n",
        "There are already pre-trained word embeddings (e.g. GloVE or Word2Vec) that you can download so that you don't have to train your embeddings all by yourself. Generally, these word embeddings are also based on specialized algorithms that do the embedding always a bit different, but we won't cover it here.\n",
        "\n",
        "How can you imagine what an embedding actually is? Well generally words that have a similar meaning in the context should be embedded next to each other. Below is an example of word embeddings in a two-dimensional space:!!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf9Z9no2XWIg",
        "colab_type": "text"
      },
      "source": [
        "Why should we even care about word embeddings? Because it is a really useful trick. If we were to feed our reviews into a neural network and just one-hot encode them we would have very sparse representations of our texts. Why? Let us have a look at the sentence \"I do my job\" in \"bag of words\" representation with a vocabulary of 1000: So a matrix that holds 1000 words (each column is one word), has four ones in it (one for I, one for do one for my and one for job) and 996 zeros. So it would be very sparse. This means that learning from it would be difficult, because we would need 1000 input neurons each representing the occurrence of a word in our sentence.\n",
        "\n",
        "In contrast if we do a word embedding we can fold these 1000 words in just as many dimensions as we want, in our case 32. This means that we just have an input vector of 32 values instead of 1000. So the word \"I\" would be some vector with values (0.4,0.5,0.2,...) and the same would happen with the other words. With word embedding like this, we just need 32 input neurons. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2oenWdXWIl",
        "colab_type": "text"
      },
      "source": [
        "## GRU\n",
        "\n",
        "The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n",
        "GRU are capable of learning the relationships between elements in an input sequence. In our case the elements are words. So our next layer is an GRU layer with 100 memory units.\n",
        "\n",
        "So our output of the embedding layer is a 200 times 32 matrix. Each word is represented through its position in those 32 dimensions. And the sequence is the 200 words that we feed into the GRU network.\n",
        "\n",
        "Finally at the end we have a dense layer with one node with a sigmoid activation as the output.\n",
        "\n",
        "Since we are going to have only the decision when the review is positive or negative we will use binary_crossentropy for the loss function. The optimizer is the standard one (adam) and the metrics are also the standard accuracy metric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4sxmuMtXWIn",
        "colab_type": "text"
      },
      "source": [
        "To summarize, our model is a simple RNN model with 1 embedding, 1 GRU and 1 dense layers.  840.001 parameters in total need to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVERKXdXWIs",
        "colab_type": "text"
      },
      "source": [
        "# Training/Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epWxRt52XWIu",
        "colab_type": "text"
      },
      "source": [
        "Train and evaluate our model\n",
        "\n",
        "We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iXD4dVEXWIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mpM_QmXWI8",
        "colab_type": "text"
      },
      "source": [
        "Once compiled, we can kick off the training process. There are two important training parameters that we have to specify — batch size and number of training epochs, which together with our model architecture determine the total training time.\n",
        "\n",
        "To train the model we simply call the fit function,supply it with the training data and also tell it which data it can use for validation. That is really useful because we have everything in one call. \n",
        "\n",
        "The training of the model in our case take a long time , because we are only running it on the CPU instead of the GPU. When the model training happens, we observe that the loss function, it constantly  going down, this shows that the model is improving. We will make the model see the dataset 7 times, defined by the epochs parameter. The batch size defines how many samples the model will see at once - in our case 64 reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eIft33nXWI-",
        "colab_type": "code",
        "outputId": "3d706d47-0709-4f6b-fd1d-41fcf0f79b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 7\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid),\n",
        "batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 24936 samples, validate on 64 samples\n",
            "Epoch 1/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.4233 - accuracy: 0.7956 - val_loss: 0.2534 - val_accuracy: 0.9531\n",
            "Epoch 2/7\n",
            "24936/24936 [==============================] - 126s 5ms/step - loss: 0.2189 - accuracy: 0.9173 - val_loss: 0.2231 - val_accuracy: 0.9219\n",
            "Epoch 3/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.1390 - accuracy: 0.9509 - val_loss: 0.2158 - val_accuracy: 0.9219\n",
            "Epoch 4/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.0995 - accuracy: 0.9660 - val_loss: 0.2475 - val_accuracy: 0.9219\n",
            "Epoch 5/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.0576 - accuracy: 0.9815 - val_loss: 0.5588 - val_accuracy: 0.8594\n",
            "Epoch 6/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.0374 - accuracy: 0.9882 - val_loss: 0.4009 - val_accuracy: 0.8594\n",
            "Epoch 7/7\n",
            "24936/24936 [==============================] - 125s 5ms/step - loss: 0.0332 - accuracy: 0.9894 - val_loss: 0.4764 - val_accuracy: 0.8906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbe782d0f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6KAjcqWXWJY",
        "colab_type": "text"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "Once we have finished training the model we can easily test its accuracy. Keras provides a very handy function to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcUQnaDMXWJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L9QlB2BXWJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAKhBr1yXWJv",
        "colab_type": "code",
        "outputId": "61150565-f22b-4e43-9f48-bbda747f9201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (acc*100))\n",
        "print(\"Test score: %.2f%%\" % (score*100))\n"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.40%\n",
            "Test score: 56.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfckRNLVXWKO",
        "colab_type": "text"
      },
      "source": [
        "In our case the model achieved an accuracy of around 80% which is excellent, given the difficult task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnvFOzVsXWJ6",
        "colab_type": "code",
        "outputId": "e44574b0-6acb-4191-fb61-5563a9a48ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "y_pred = model.predict_classes(np.array(X_test))\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, y_pred))\n",
        "target_names = ['pos', 'neg']\n",
        "cnf_matrix_test = confusion_matrix(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "print(cnf_matrix_test)\n",
        "df_cm = pd.DataFrame(cnf_matrix_test, range(2), range(2))\n",
        "sn.set(font_scale=1.4)\n",
        "sn.heatmap(df_cm, annot=True, fmt='d')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.85396\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         pos       0.85      0.85      0.85     12500\n",
            "         neg       0.85      0.85      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "[[10686  1814]\n",
            " [ 1837 10663]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbe7c78b080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLuE0BNLu2f",
        "colab_type": "text"
      },
      "source": [
        "According to the confusion matrix, 10663 positive reviews were correctly predicted (True Positive) and 10686 negative samples were correctly predicted (True Negative) . And number of incorrect predictions are 1837 (False Positive)  and 1814 (False Negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc9CporWXWKB",
        "colab_type": "code",
        "outputId": "6457b952-ed22-4b93-900d-fc5da2b1be5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "TP = cnf_matrix_test[1, 1]\n",
        "TN = cnf_matrix_test[0, 0]\n",
        "FP = cnf_matrix_test[0, 1]\n",
        "FN = cnf_matrix_test[1, 0]\n",
        "\n",
        "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
        "precision=TP/float(TP+FP)\n",
        "print('classification error:')\n",
        "print(classification_error)\n",
        "print('precision:')\n",
        "print(precision)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classification error:\n",
            "0.14604\n",
            "precision:\n",
            "0.8546124869760359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mZVKThVM5Xc",
        "colab_type": "text"
      },
      "source": [
        "Misclassification rate is calculated and found as 0.14768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uuF9ibtSBfT",
        "colab_type": "text"
      },
      "source": [
        "#save model to disk\n",
        "JSON is a simple file format for describing data hierarchically.\n",
        "\n",
        "Keras provides the ability to describe any model using JSON format with a to_json() function. This can be saved to file and later loaded via the model_from_json() function that will create a new model from the JSON specification.\n",
        "\n",
        "The weights are saved directly from the model using the save_weights() function and later loaded using the symmetrical load_weights() function.\n",
        "\n",
        "The example below trains and evaluates our model on the IMDB dataset. The model is then converted to JSON format and written to model.json in the local directory. The network weights are written to model.h5 in the local directory.\n",
        "\n",
        "The model and weight data is loaded from the saved files and a new model is created. It is important to compile the loaded model before it is used. This is so that predictions made using the model can use the appropriate efficient computation from the Keras backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El5mxNnZPJRE",
        "colab_type": "code",
        "outputId": "938900c9-80aa-4efb-d9f2-0935adbfeea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "#serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"saved model to disk\")\n",
        "\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AI_YXxgXWKO",
        "colab_type": "text"
      },
      "source": [
        "## Predict something\n",
        "\n",
        "Of course at the end we want to use our model in an application. So we want to use it to create predictions. In order to do so we need to translate our sentence into the corresponding word integers and then pad it to match our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EccDSoKc-9sM",
        "colab_type": "code",
        "outputId": "4a06711b-9b3f-40aa-8ce8-916153991e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#predicting for new datasets\n",
        "from keras.preprocessing import text\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "json_file = open('model.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "#load weights into new model\n",
        "loaded_model.load_weights(\"gru_model.h5\")\n",
        "\n",
        "#compile and evaluate loaded model\n",
        "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=2500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=' ')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhExeXiYXWKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this has to be loaded for new text conversion into vectors\n",
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX91zI5Y-1d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment(text):\n",
        "    tokenizedText= text_to_word_sequence(text)\n",
        "    numericText = np.array([word_index[word] if (word in word_index) and (word_index[word]<25000) else 0 for word in tokenizedText])\n",
        "    numeric_inp = sequence.pad_sequences([numericText],maxlen=max_words)\n",
        "    out = loaded_model.predict(numeric_inp)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dKeAtSD6N3d",
        "colab_type": "text"
      },
      "source": [
        "now we will predict some opinion on social media\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JyETR5xKiF7",
        "colab_type": "code",
        "outputId": "9a67084c-6d55-4196-dbb5-1b473321cb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#positive opinion\n",
        "doc1=\"I think social media is a great tool to have access to. I love how convenient it makes it to stay in contact with friends and family. On the downside, I don't think people value face-to-face interactions as much anymore. I also think people can be much ruder/meaner when hiding behind a keyboard.\"\n",
        "print(sentiment(doc1))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9990151]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EBOansA61KW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6db34239-fde5-4ff1-94de-c7ffae841d6a"
      },
      "source": [
        "#positive opinion\n",
        "doc2=\"People use social media to vent about everything from customer service to politics, but the downside to this is that our feeds often resemble an endless stream of stress. In 2015, researchers at the Pew Research Center based in Washington DC sought to find out if social media induces more stress than it relieves. \"\n",
        "print(sentiment(doc2))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03993706]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AsJHvpYXWKj",
        "colab_type": "text"
      },
      "source": [
        "In this case a value close to 0 means the sentiment was negative and a value close to 1 means its a positive review. You can also use \"model.predict_classes\" to just get the classes of positive and negative. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Mi2nnsXWKl",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion or what’s next?\n",
        "\n",
        "So we have built quite a cool sentiment analysis for IMDB reviews that predicts if a movie review is positive or negative with 90% accuracy. With this we are already quite close to industry standards.The big benefit while comparing our self-built solution with an SaaS solution on the market is that we own our data and model. We can now deploy this model on our own infrastructure and use it as often as we like. Google or Amazon never get to see sensitive customer data, which might be relevant for certain business cases. We can train it with Arabic or other language given that we find a nice dataset, or simply build one ourselves. "
      ]
    }
  ]
}